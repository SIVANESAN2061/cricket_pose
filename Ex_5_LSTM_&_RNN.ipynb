{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiPjZ9AdrTyq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual dataset loading logic\n",
        "def load_dataset(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_index, folder_name in enumerate(os.listdir(dataset_path)):\n",
        "        folder_path = os.path.join(dataset_path, folder_name)\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            for filename in os.listdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, filename)\n",
        "                image = cv2.imread(image_path)\n",
        "\n",
        "                if image is not None:\n",
        "                    # Resize images to a consistent size if needed\n",
        "                    image = cv2.resize(image, (width, height))  # Set your desired width and height\n",
        "                    images.append(image)\n",
        "                    labels.append(class_index)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/data\"  # Replace with your dataset path\n",
        "width, height = 64, 64  # Set your desired width and height\n",
        "X, y = load_dataset(dataset_path)"
      ],
      "metadata": {
        "id": "kvrDw68qrgXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opencv-python scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NadbAI3roSyf",
        "outputId": "077c5f8b-901e-41ec-b1b6-f87575932dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this with your actual model training logic for LSTM\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val, epochs):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
        "    return history\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], -1))\n",
        "\n",
        "# Train the LSTM model\n",
        "epochs = 20  # Set the number of epochs\n",
        "history = train_lstm_model(X_train, y_train, X_val, y_val, epochs)\n",
        "\n",
        "# Print the accuracy history\n",
        "print(\"Accuracy History:\")\n",
        "for epoch, acc in enumerate(history.history['accuracy']):\n",
        "    print(f\"Epoch {epoch + 1}: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSCvmtTvs-Ry",
        "outputId": "25e2d079-3282-427b-eae2-1664843d440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "170/170 [==============================] - 44s 218ms/step - loss: 1.5924 - accuracy: 0.2891 - val_loss: 1.5793 - val_accuracy: 0.3076\n",
            "Epoch 2/20\n",
            "170/170 [==============================] - 39s 230ms/step - loss: 1.5827 - accuracy: 0.3066 - val_loss: 1.5810 - val_accuracy: 0.3076\n",
            "Epoch 3/20\n",
            "170/170 [==============================] - 38s 221ms/step - loss: 1.5804 - accuracy: 0.3082 - val_loss: 1.5825 - val_accuracy: 0.3076\n",
            "Epoch 4/20\n",
            "170/170 [==============================] - 39s 231ms/step - loss: 1.5791 - accuracy: 0.3075 - val_loss: 1.5731 - val_accuracy: 0.3076\n",
            "Epoch 5/20\n",
            "170/170 [==============================] - 36s 209ms/step - loss: 1.5785 - accuracy: 0.3066 - val_loss: 1.5747 - val_accuracy: 0.3076\n",
            "Epoch 6/20\n",
            "170/170 [==============================] - 36s 209ms/step - loss: 1.5772 - accuracy: 0.3075 - val_loss: 1.5739 - val_accuracy: 0.3068\n",
            "Epoch 7/20\n",
            "170/170 [==============================] - 38s 226ms/step - loss: 1.5748 - accuracy: 0.3082 - val_loss: 1.5836 - val_accuracy: 0.3076\n",
            "Epoch 8/20\n",
            "170/170 [==============================] - 36s 209ms/step - loss: 1.5754 - accuracy: 0.3073 - val_loss: 1.5710 - val_accuracy: 0.3076\n",
            "Epoch 9/20\n",
            "170/170 [==============================] - 36s 213ms/step - loss: 1.5720 - accuracy: 0.3064 - val_loss: 1.5800 - val_accuracy: 0.3076\n",
            "Epoch 10/20\n",
            "170/170 [==============================] - 37s 216ms/step - loss: 1.5762 - accuracy: 0.3077 - val_loss: 1.5744 - val_accuracy: 0.3076\n",
            "Epoch 11/20\n",
            "170/170 [==============================] - 41s 240ms/step - loss: 1.5730 - accuracy: 0.3075 - val_loss: 1.5663 - val_accuracy: 0.3076\n",
            "Epoch 12/20\n",
            "170/170 [==============================] - 43s 255ms/step - loss: 1.5737 - accuracy: 0.3070 - val_loss: 1.5698 - val_accuracy: 0.3076\n",
            "Epoch 13/20\n",
            "170/170 [==============================] - 45s 266ms/step - loss: 1.5719 - accuracy: 0.3070 - val_loss: 1.5684 - val_accuracy: 0.3076\n",
            "Epoch 14/20\n",
            "170/170 [==============================] - 44s 257ms/step - loss: 1.5694 - accuracy: 0.3075 - val_loss: 1.5676 - val_accuracy: 0.3076\n",
            "Epoch 15/20\n",
            "170/170 [==============================] - 37s 219ms/step - loss: 1.5675 - accuracy: 0.3090 - val_loss: 1.5687 - val_accuracy: 0.3083\n",
            "Epoch 16/20\n",
            "170/170 [==============================] - 41s 242ms/step - loss: 1.5644 - accuracy: 0.3060 - val_loss: 1.5744 - val_accuracy: 0.3076\n",
            "Epoch 17/20\n",
            "170/170 [==============================] - 41s 240ms/step - loss: 1.5755 - accuracy: 0.3073 - val_loss: 1.5776 - val_accuracy: 0.3076\n",
            "Epoch 18/20\n",
            "170/170 [==============================] - 37s 217ms/step - loss: 1.5740 - accuracy: 0.3071 - val_loss: 1.5703 - val_accuracy: 0.3076\n",
            "Epoch 19/20\n",
            "170/170 [==============================] - 40s 234ms/step - loss: 1.5750 - accuracy: 0.3071 - val_loss: 1.5749 - val_accuracy: 0.3076\n",
            "Epoch 20/20\n",
            "170/170 [==============================] - 38s 220ms/step - loss: 1.5715 - accuracy: 0.3070 - val_loss: 1.5667 - val_accuracy: 0.3076\n",
            "Accuracy History:\n",
            "Epoch 1: 0.289105623960495\n",
            "Epoch 2: 0.30658814311027527\n",
            "Epoch 3: 0.30824437737464905\n",
            "Epoch 4: 0.30750828981399536\n",
            "Epoch 5: 0.30658814311027527\n",
            "Epoch 6: 0.30750828981399536\n",
            "Epoch 7: 0.30824437737464905\n",
            "Epoch 8: 0.30732426047325134\n",
            "Epoch 9: 0.30640411376953125\n",
            "Epoch 10: 0.3076923191547394\n",
            "Epoch 11: 0.30750828981399536\n",
            "Epoch 12: 0.3069562017917633\n",
            "Epoch 13: 0.3069562017917633\n",
            "Epoch 14: 0.30750828981399536\n",
            "Epoch 15: 0.3089804947376251\n",
            "Epoch 16: 0.3060360550880432\n",
            "Epoch 17: 0.30732426047325134\n",
            "Epoch 18: 0.3071402311325073\n",
            "Epoch 19: 0.3071402311325073\n",
            "Epoch 20: 0.3069562017917633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "# Replace this with your actual model training logic for LSTM\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val, epochs):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # Use EarlyStopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "    return history\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], -1))\n",
        "\n",
        "# Train the LSTM model\n",
        "epochs = 10  # Set the number of epochs\n",
        "history = train_lstm_model(X_train, y_train, X_val, y_val, epochs)\n",
        "\n",
        "# Print the accuracy history\n",
        "print(\"Accuracy History:\")\n",
        "for epoch, acc in enumerate(history.history['accuracy']):\n",
        "    print(f\"Epoch {epoch + 1}: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6AdrTqPsquq",
        "outputId": "af10e305-17af-4cb4-c20f-6f8e8e1be050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "170/170 [==============================] - 39s 201ms/step - loss: 1.7372 - accuracy: 0.2575 - val_loss: 1.5732 - val_accuracy: 0.3076\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 32s 188ms/step - loss: 1.6539 - accuracy: 0.2644 - val_loss: 1.6088 - val_accuracy: 0.3032\n",
            "Epoch 3/10\n",
            "170/170 [==============================] - 38s 221ms/step - loss: 1.6320 - accuracy: 0.2560 - val_loss: 1.5890 - val_accuracy: 0.3083\n",
            "Epoch 4/10\n",
            "170/170 [==============================] - 31s 181ms/step - loss: 1.6134 - accuracy: 0.2827 - val_loss: 1.6541 - val_accuracy: 0.1670\n",
            "Epoch 5/10\n",
            "170/170 [==============================] - 30s 178ms/step - loss: 1.6090 - accuracy: 0.2830 - val_loss: 1.5730 - val_accuracy: 0.3098\n",
            "Epoch 6/10\n",
            "170/170 [==============================] - 32s 187ms/step - loss: 1.5982 - accuracy: 0.2816 - val_loss: 1.5953 - val_accuracy: 0.2701\n",
            "Epoch 7/10\n",
            "170/170 [==============================] - 32s 188ms/step - loss: 1.5906 - accuracy: 0.2856 - val_loss: 1.5637 - val_accuracy: 0.3216\n",
            "Epoch 8/10\n",
            "170/170 [==============================] - 38s 221ms/step - loss: 1.5765 - accuracy: 0.3001 - val_loss: 1.6015 - val_accuracy: 0.1943\n",
            "Epoch 9/10\n",
            "170/170 [==============================] - 32s 188ms/step - loss: 1.5738 - accuracy: 0.2981 - val_loss: 1.5383 - val_accuracy: 0.3252\n",
            "Epoch 10/10\n",
            "170/170 [==============================] - 33s 194ms/step - loss: 1.5636 - accuracy: 0.3169 - val_loss: 1.5377 - val_accuracy: 0.3223\n",
            "Accuracy History:\n",
            "Epoch 1: 0.3075791001319885\n",
            "Epoch 2: 0.3031640946865082\n",
            "Epoch 3: 0.3083149492740631\n",
            "Epoch 4: 0.1670345813035965\n",
            "Epoch 5: 0.3097866177558899\n",
            "Epoch 6: 0.270051509141922\n",
            "Epoch 7: 0.32155996561050415\n",
            "Epoch 8: 0.19426049292087555\n",
            "Epoch 9: 0.3252391517162323\n",
            "Epoch 10: 0.32229581475257874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "# Replace this with your actual model training logic for LSTM\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val, epochs):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(len(np.unique(y_train)), activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # One-hot encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "    y_val_encoded = le.transform(y_val)\n",
        "\n",
        "    # Reshape input data for LSTM\n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "    X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_reshaped, y_train_encoded, epochs=epochs, validation_data=(X_val_reshaped, y_val_encoded))\n",
        "\n",
        "    return history\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], -1))\n",
        "\n",
        "# Train the LSTM model\n",
        "epochs = 10  # Set the number of epochs\n",
        "history = train_lstm_model(X_train, y_train, X_val, y_val, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ-iLXKDwIfH",
        "outputId": "796ef807-ba5e-46d5-da73-aba7c097f0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "170/170 [==============================] - 28s 150ms/step - loss: 1.5839 - accuracy: 0.3033 - val_loss: 1.6103 - val_accuracy: 0.3054\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 23s 133ms/step - loss: 1.5686 - accuracy: 0.3093 - val_loss: 1.5790 - val_accuracy: 0.3054\n",
            "Epoch 3/10\n",
            "170/170 [==============================] - 25s 148ms/step - loss: 1.5681 - accuracy: 0.3121 - val_loss: 1.5690 - val_accuracy: 0.3113\n",
            "Epoch 4/10\n",
            "170/170 [==============================] - 22s 132ms/step - loss: 1.5769 - accuracy: 0.3088 - val_loss: 1.5808 - val_accuracy: 0.3076\n",
            "Epoch 5/10\n",
            "170/170 [==============================] - 26s 156ms/step - loss: 1.5656 - accuracy: 0.3119 - val_loss: 1.5603 - val_accuracy: 0.3135\n",
            "Epoch 6/10\n",
            "170/170 [==============================] - 22s 131ms/step - loss: 1.5697 - accuracy: 0.3084 - val_loss: 1.5676 - val_accuracy: 0.3164\n",
            "Epoch 7/10\n",
            "170/170 [==============================] - 23s 138ms/step - loss: 1.5685 - accuracy: 0.3097 - val_loss: 1.5804 - val_accuracy: 0.3135\n",
            "Epoch 8/10\n",
            "170/170 [==============================] - 29s 168ms/step - loss: 1.5633 - accuracy: 0.3108 - val_loss: 1.5788 - val_accuracy: 0.3091\n",
            "Epoch 9/10\n",
            "170/170 [==============================] - 22s 127ms/step - loss: 1.5646 - accuracy: 0.3088 - val_loss: 1.5612 - val_accuracy: 0.3098\n",
            "Epoch 10/10\n",
            "170/170 [==============================] - 22s 132ms/step - loss: 1.5690 - accuracy: 0.3123 - val_loss: 1.5681 - val_accuracy: 0.3135\n"
          ]
        }
      ]
    }
  ]
}